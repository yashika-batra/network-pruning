{"cells":[{"cell_type":"markdown","metadata":{"id":"y-eC-sb34T9w"},"source":["## Accelerate Inference: Neural Network Pruning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L47XBZWm4T9x"},"outputs":[],"source":["import os\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import pickle\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import datasets, layers, models, regularizers\n","from tensorflow.keras.layers import *\n","\n","print(tf.version.VERSION)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V1FQTVeAuNiU"},"outputs":[],"source":["# untar\n","!ls\n","!tar -xvzf dataset.tar.gz\n","# load train\n","train_images = pickle.load(open('train_images.pkl', 'rb'))\n","train_labels = pickle.load(open('train_labels.pkl', 'rb'))\n","# load val\n","val_images = pickle.load(open('val_images.pkl', 'rb'))\n","val_labels = pickle.load(open('val_labels.pkl', 'rb'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KE9JuZDG4T94"},"outputs":[],"source":["# Define the neural network architecture (don't change this)\n","\n","model = models.Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5), input_shape=(25,25,3)))\n","model.add(Activation('relu'))\n","model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(1e-5)))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5)))\n","model.add(Activation('relu'))\n","model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(1e-5)))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","model.add(Flatten())\n","model.add(Dense(512))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(5))\n","model.add(Activation('softmax'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTzcSoYl4T97"},"outputs":[],"source":["print(model.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9Nk_MAPqZPt"},"outputs":[],"source":["# you can use the default hyper-parameters for training,\n","# val accuracy ~72% after 50 epochs\n","\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001, weight_decay=1e-6),\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","              metrics=['accuracy'])\n","\n","history = model.fit(train_images, train_labels, batch_size=32, epochs=50,\n","                    validation_data=(val_images, val_labels)) # train for 50 epochs, with batch size 32"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vOhpP7M24T9_"},"outputs":[],"source":["original_weights = model.get_weights()\n","results = model.evaluate(val_images, val_labels, batch_size=128)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjw94aij4T-C"},"outputs":[],"source":["class Pruner:\n","  def __init__(self, model):\n","    self.model = model\n","    weights = model.get_weights()\n","    self.kernels = [weights[0], weights[2], weights[4], weights[6]]\n","\n","\n","  def prune(self, prune_rate):\n","    # for each layer, prune everything below a certain threshold\n","    norms = []\n","    for i in range(len(self.kernels)):\n","      weights = self.kernels[i]\n","      norms.append([])\n","      # flatten kernel, get number of filters\n","      num_filters, num_channels, dim_a, dim_b = np.shape(weights)\n","      filters = tf.cast(tf.reshape(weights, [num_filters, num_channels * dim_a * dim_b]), dtype=tf.float32)\n","      for f in filters:\n","        # take the L1 norm of each flattened filter and add to norms\n","        l1_norm = np.sum(tf.math.abs(f).numpy())\n","        norms[i].append(l1_norm)\n","\n","    # get all filters below prune_rate percentile\n","    norms = np.array(norms)\n","    threshold = np.percentile(norms.flatten(), prune_rate)\n","    norms = tf.convert_to_tensor(norms)\n","    greater_filters = tf.cast(tf.math.greater(norms, threshold), dtype=tf.float32).numpy()\n","\n","    # set entire filters that are not greater than threshold to 0\n","    conv_masks = []\n","    for i in range(len(self.kernels)):\n","      layer = self.kernels[i]\n","      curr_mask = []\n","      for j in range(layer.shape[0]):\n","        filter = layer[j]\n","        # stack everything\n","        curr_mask.append(greater_filters[i][j] * np.ones(filter.shape))\n","      curr_mask = np.array(curr_mask)\n","      conv_masks.append(curr_mask)\n","\n","    all_weights = model.get_weights()\n","    all_masks = [np.ones(x.shape) for x in all_weights]\n","    all_masks[0] = conv_masks[0]\n","    all_masks[2] = conv_masks[1]\n","    all_masks[4] = conv_masks[2]\n","    all_masks[6] = conv_masks[3]\n","    new_weights = [tf.math.multiply(all_masks[i], all_weights[i]) for i in range(len(all_masks))]\n","\n","    self.weights = new_weights\n","    self.masks = all_masks\n","\n","    # get sparsity\n","    # for each layer\n","    self.total_parameters = 0\n","    num_one_weights = 0\n","    for mask in self.masks:\n","      num_one_weights += np.sum(mask)\n","      self.total_parameters += np.prod(np.array(mask.shape))\n","    self.num_zero_weights = self.total_parameters - num_one_weights\n","\n","\n","  def fine_tune(self):\n","    \"\"\"\n","    training loop adapted from keras documentation:\n","    https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\n","    \"\"\"\n","\n","    # Instantiate an optimizer.\n","    optimizer = keras.optimizers.Adam(learning_rate=1e-6, weight_decay=1e-8)\n","    # Instantiate a loss function.\n","    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n","\n","    # Prepare the training dataset.\n","    batch_size = 32\n","    train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n","    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n","\n","    # Prepare the validation dataset.\n","    val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n","    val_dataset = val_dataset.batch(batch_size)\n","\n","    # Prepare the metrics.\n","    train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n","    val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n","\n","    epochs = 20\n","    for epoch in range(epochs):\n","      # Iterate over the batches of the dataset.\n","      for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n","        # Open a GradientTape to record the operations run\n","        # during the forward pass, which enables auto-differentiation.\n","        with tf.GradientTape() as tape:\n","            # Run the forward pass of the layer. The operations that the layer applies\n","            # to its inputs are going to be recorded on the GradientTape.\n","            logits = self.model(x_batch_train, training=True)  # Logits for this minibatch\n","            # Compute the loss value for this minibatch.\n","            loss_value = loss_fn(y_batch_train, logits)\n","\n","        # Use the gradient tape to automatically retrieve\n","        # the gradients of the trainable variables with respect to the loss.\n","        grads = tape.gradient(loss_value, self.model.trainable_weights)\n","\n","        # Run one step of gradient descent by updating\n","        # the value of the variables to minimize the loss.\n","        optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n","\n","\n","      # add masks to the trainable_weights\n","      pre_masked_weights = self.model.get_weights()\n","      masked_weights = []\n","      for i in range(len(pre_masked_weights)):\n","        weight = pre_masked_weights[i]\n","        mask = self.masks[i]\n","        masked_weights.append(tf.math.multiply(weight, mask))\n","\n","      self.weights = masked_weights\n","      self.model.set_weights(self.weights)\n","\n","      # Run a validation loop at the end of each epoch.\n","      for x_batch_val, y_batch_val in val_dataset:\n","          val_logits = model(x_batch_val, training=False)\n","          # Update val metrics\n","          val_acc_metric.update_state(y_batch_val, val_logits)\n","      val_acc = val_acc_metric.result()\n","      val_acc_metric.reset_states()\n","      print(\"Epoch %d\" % (epoch,), \"Validation acc: %.4f\" % (float(val_acc),))\n","\n","\n","    # count number of zero weights for each layer at the end of all training\n","    num_zero_weights = 0\n","    for weights in self.weights:\n","      zero_weights = tf.cast(tf.math.equal(weights, 0.0), dtype=tf.float32)\n","      num_zero_weights += np.sum(zero_weights.numpy())\n","    self.num_zero_weights = num_zero_weights\n","    print(\"sparsity:\", self.num_zero_weights/self.total_parameters)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZWl3hIh66Ce"},"outputs":[],"source":["def test_pruning(prune_rate):\n","  # reset the model weights to the original values\n","  model.set_weights(original_weights)\n","\n","  # create the pruner, prune, set weights\n","  pruner = Pruner(model)\n","  pruner.prune(prune_rate)\n","  model.set_weights(pruner.weights)\n","  pruner.fine_tune()\n","  model.set_weights(pruner.weights)\n","\n","  # evaluate final model and get the accuracy\n","  results = model.evaluate(val_images, val_labels, batch_size=128)\n","  accuracy = results[1]\n","\n","  # get the pruning score\n","  print(\"sparsity\", pruner.num_zero_weights / pruner.total_parameters)\n","  if accuracy > 0.6 and prune_rate > 0:\n","    return (accuracy + pruner.num_zero_weights / pruner.total_parameters) / 2, pruner.weights\n","  else:\n","    return 0, pruner.weights\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EqEX8MB_rATn"},"outputs":[],"source":["new_finetune_tests = [5]\n","for rate in new_finetune_tests:\n","  score_metric, finetune_weights = test_pruning(rate)\n","  model.set_weights(finetune_weights)\n","  print(\"\\n---post fine-tuning---\\n\")\n","  print(\"best score:\", score_metric, \"\\nbest prune rate:\", rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMSKQW4k4T-G"},"outputs":[],"source":["# you need to save the model's weights, naming it 'my_model_weights.h5'\n","model.save_weights(\"my_model_weights_fil.h5\")\n","\n","# running this cell will immediately download a file called 'my_model_weights.h5'\n","from google.colab import files\n","files.download(\"my_model_weights_fil.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mn28cuhh9dUh"},"outputs":[],"source":["# pareto frontier plot\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# define data values\n","x = np.array([0.00048572098365245314, 0.0009714419673049063, 0.0014571629509573594,\n","              0.004315833323495235, 0.0066381867765835266]) * 100 # X-axis points\n","y = np.array([0.4317, 0.3873, 0.1921, 0.1921, 0.1921]) * 100\n","plt.xlabel(\"Sparsity (%)\")\n","plt.ylabel(\"Accuracy (%)\")\n","plt.title(\"Filter Pruning Pareto Frontier: Accuracy vs. Sparsity\")\n","\n","plt.plot(x, y, 'o')  # Plot the chart\n","plt.grid()\n","plt.show()  # display"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1hBj7JFE3RsOh_FQTKPmKKbE2ksNaaUSp","timestamp":1713317822749},{"file_id":"1Biq29BhmywXIzRURFDu-APUwClXE8rmu","timestamp":1712331386356},{"file_id":"1w20Vhx_MxwY8mVmLORuQcAMSpCdIKmGb","timestamp":1662737366034},{"file_id":"1-kM27DCVOvQ0iPbvNqvHyMNpEBkagy9Q","timestamp":1603749902126},{"file_id":"1TVz0yWqJXl98n1Jnrs-IkBe2fnniRg_9","timestamp":1603747597083}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}